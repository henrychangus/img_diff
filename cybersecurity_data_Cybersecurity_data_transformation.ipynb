{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ccec4d-de53-4e8a-b628-2746179727cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/henrychang/sys_two_ai\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "gpt-3.5-turbo is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt-3.5-turbo/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6772a3ba-77c27eec6ec6d02c41f33a13;12b9249d-f204-462b-b51b-f79b6f0c9dec)\n\nRepository Not Found for url: https://huggingface.co/gpt-3.5-turbo/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 795\u001b[0m\n\u001b[1;32m    792\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m current_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/input/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    793\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m current_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/output/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 795\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mCybersecurityDataTransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/input_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/detection_results.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/missing_dict.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/vectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/isolation_forest_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/scaler.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/label_encoders.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/aggregated_features.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine_tune_id.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m   detector\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mCybersecurityDataTransformation.__init__\u001b[0;34m(self, input_file, output_file, missing_dict_file, vectorizer_file, model_file, scaler_file, label_encoders_file, aggregated_features_file, fine_tune_id_file, chunk_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoders \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregated_features \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/__init__.py:812\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: gpt-3.5-turbo is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AI Agent for Data Transformation:\n",
    "Combining traditional machine learning (ML) techniques with modern Large Language Models (LLMs) \n",
    "enhances the detection and analysis of security threats in log data.\n",
    "\n",
    "Hybrid Approach:\n",
    "    Traditional ML Techniques: \n",
    "        Methods like Isolation Forests, clustering algorithms, and statistical analysis identify \n",
    "    patterns and outliers in structured data.\n",
    "    Large Language Models: \n",
    "        Advanced LLMs (e.g., GPT-3.5 Turbo, GPT-4, or their fin-tuned versions) excel at parsing \n",
    "        and interpreting complex log entries, generating contextual insights and responses.\n",
    "        \n",
    "Data Processing:\n",
    "    Stream-based Approach:\n",
    "        Data are read in and processed sequentially. For example, by our proprietary algorithm, \n",
    "        the aggregation features can be prepared efficiently in real-time.\n",
    "    Filtering Non-Threat Data: \n",
    "        Non-relevant data is filtered out initially to reduce costs.\n",
    "    Data Augmentation: \n",
    "        Augmentation is provided as needed for better predictive performance.\n",
    "        \n",
    "By integrating these approaches, the AI agent enhances threat detection accuracy through \n",
    "a combination of numerical and contextual analysis.\n",
    "'''\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import openai\n",
    "import joblib\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the desired working directory \n",
    "desired_directory = '/Users/henrychang/sys_two_ai' \n",
    "os.chdir(desired_directory) \n",
    "# Verify the change \n",
    "current_directory = os.getcwd() \n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "class CybersecurityDataTransformation:\n",
    "    \"\"\"\n",
    "    A class to detect cybersecurity threats using various machine learning models and AI data processing techniques.\n",
    " \n",
    "    Attributes:\n",
    "        input_file (str): Path to the input file.\n",
    "        output_file (str): Path to the output file.\n",
    "        vectorizer_file (str): Path to the vectorizer file.\n",
    "        model_file (str): Path to the model file.\n",
    "        scaler_file (str): Path to the scaler file.\n",
    "        label_encoders_file (str): Path to the label encoders file.\n",
    "        aggregated_features_file (str): Path to the aggregated features file.\n",
    "        fine_tune_id_file (str): Path to the fine-tuned model ID file.\n",
    "        chunk_size (int): Size of data chunks to process. Default is 1000.\n",
    "\n",
    "    Ensure these parameters are correctly included in the init method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_file, output_file, missing_dict_file, vectorizer_file, model_file, scaler_file, label_encoders_file, aggregated_features_file, fine_tune_id_file, chunk_size=1000):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.missing_dict_file = missing_dict_file\n",
    "        self.vectorizer_file = vectorizer_file\n",
    "        self.model_file = model_file\n",
    "        self.scaler_file = scaler_file\n",
    "        self.label_encoders_file = label_encoders_file\n",
    "        self.aggregated_features_file = aggregated_features_file\n",
    "        self.fine_tune_id_file = fine_tune_id_file\n",
    "        self.chunk_size = chunk_size\n",
    "        self.df = pd.DataFrame()\n",
    "        self.missing_dict = {}\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.isolation_forest = IsolationForest()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.aggregated_features = {}\n",
    "        openai.api_key = 'MY_OPENAI_API_KEY' # Set the OpenAI API key\n",
    "        # If using the Hugging Face transformers library and the models are available locally or \n",
    "        # downloaded from the Hugging Face model hub, we do not need an API key.\n",
    "        # Initialize the generative model. If resources are limited, use DistilGPT-2; \n",
    "        # otherwise, use GPT-4. GPT-3.5 Turbo is a balanced option.\n",
    "        self.text_generator = pipeline('text-generation', model='gpt-3.5-turbo') \n",
    "\n",
    "\n",
    "    def load_fine_tuned_model(self):\n",
    "        \"\"\"\n",
    "        Loads the fine-tuned model using the saved fine_tune_id.\n",
    "\n",
    "        Returns:\n",
    "            str: The fine-tuned model identifier.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the fine-tuned model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the fine-tune ID from the file\n",
    "            with open(self.fine_tune_id_file, 'r') as f:\n",
    "                fine_tune_id = f.read().strip()\n",
    "            logger.info(f\"Fine-tune ID read from file: {fine_tune_id}\")\n",
    "\n",
    "            # Retrieve the fine-tune response using the fine-tune ID\n",
    "            fine_tune_response = openai.FineTune.retrieve(id=fine_tune_id)\n",
    "            fine_tuned_model = fine_tune_response['fine_tuned_model']\n",
    "            logger.info(f\"Fine-tuned model loaded: {fine_tuned_model}\")\n",
    "\n",
    "            return fine_tuned_model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading fine-tuned model: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "\n",
    "    def load_preprocessors(self):\n",
    "        \"\"\"\n",
    "        Loads the vectorizer, scaler, and label encoders from their respective files.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading any of the preprocessors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = joblib.load(self.vectorizer_file)\n",
    "            self.scaler = joblib.load(self.scaler_file)\n",
    "            self.label_encoders = joblib.load(self.label_encoders_file)\n",
    "            logger.info(\"Vectorizer, scaler, and label encoders loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading preprocessors: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    def load_isolation_forest(self):\n",
    "        \"\"\"\n",
    "        Loads the trained Isolation Forest model from the model file.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the Isolation Forest model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.isolation_forest = joblib.load(self.model_file)\n",
    "            logger.info(\"Trained Isolation Forest model loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading trained Isolation Forest model: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    def load_aggregated_features(self):\n",
    "        \"\"\"\n",
    "        Loads the aggregated features dictionary from the aggregated features file.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the aggregated features dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.aggregated_features = joblib.load(self.aggregated_features_file)\n",
    "            logger.info(\"Aggregated features dictionary loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading aggregated features dictionary: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "\n",
    "    def load_missing_data_replacement(self):\n",
    "        \"\"\"\n",
    "        Loads the dictionary containing precomputed values for missing data.\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the missing values dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dictionary containing precomputed values for missing data using joblib \n",
    "            self.missing_dict = joblib.load(self.missing_dict_file) \n",
    "            logger.info(\"Missing data replacement dictionary loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.missing_dict_file}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading file: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    '''\n",
    "    Key Points to Consider\n",
    "    Data Distribution: Choose the imputation method based on the distribution of the data.\n",
    "    Impact on Analysis: Consider how the imputed values might affect subsequent analyses and model performance.\n",
    "    Consistency: Ensure that the imputation method is applied consistently across similar datasets.\n",
    "    '''\n",
    "    def preprocess_data(self, chunk):\n",
    "        \"\"\"\n",
    "        Preprocesses the data chunk by filling missing values using precomputed values from a dictionary.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to preprocess.\n",
    "        Returns:\n",
    "            DataFrame: The preprocessed data chunk.\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs during preprocessing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Iterate through each column in the chunk to fill missing values\n",
    "            for col in chunk.columns:\n",
    "                # Numeric columns, Categorical columns, Boolean columns\n",
    "                if chunk[col].dtype in ['float64', 'int64', 'object', 'bool']:\n",
    "                    chunk[col].fillna(self.missing_dict[col], inplace=True)  # Use precomputed value\n",
    "                elif pd.api.types.is_datetime64_any_dtype(chunk[col]):  # Datetime columns\n",
    "                    chunk[col].fillna(self.missing_dict[col], inplace=True)  # Use precomputed value\n",
    "            logger.info(\"Data preprocessed\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during preprocessing: {e}\")\n",
    "            sys.exit(1)  # Exit the program if preprocessing fails\n",
    "\n",
    "\n",
    "    '''\n",
    "    Remove rows that are obviously not threats.\n",
    "    '''\n",
    "    def simple_filter(self, chunk):\n",
    "        \"\"\"\n",
    "        Applies a simple rule-based filter to remove non-threat rows based on certain conditions.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The filtered data chunk.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns the original chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define conditions to filter out non-threat rows \n",
    "            filter_conditions = [ 'login successful', \n",
    "                                 'system update', \n",
    "                                 'user logged in', \n",
    "                                 'user logged out', \n",
    "                                 'system rebooted', \n",
    "                                 'file saved', \n",
    "                                 'file opened', \n",
    "                                 'file closed', \n",
    "                                 'session ended', \n",
    "                                 'session started', \n",
    "                                 'heartbeat message', \n",
    "                                 'backup completed', \n",
    "                                 'scheduled task completed', \n",
    "                                 'configuration updated' ]\n",
    "\n",
    "            # Apply the filter conditions to the chunk\n",
    "            chunk = chunk[~chunk['log_text'].str.contains('|'.join(filter_conditions), case=False)]\n",
    "            logger.info(\"Simple filtering applied\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during simple filtering: {e}\")\n",
    "            return chunk\n",
    "\n",
    "    def vectorize_data(self, chunk):\n",
    "        \"\"\"\n",
    "        Vectorizes the text data in the given data chunk using the pre-trained vectorizer.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            sparse matrix: The vectorized text data.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract text data from the chunk\n",
    "            text_data = chunk['log_text']\n",
    "\n",
    "            # Transform the text data using the pre-trained vectorizer\n",
    "            vectorized_data = self.vectorizer.transform(text_data)\n",
    "            return vectorized_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during vectorization: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    '''\n",
    "    The exact range of these scores depends on the data and the\n",
    "    specifics of the Isolation Forest model, but they often fall\n",
    "    within a range approximately between -1 and 1, where:\n",
    "\n",
    "    Scores closer to 0: Indicate points that are somewhat\n",
    "    isolated but not extreme.\n",
    "    Strong negative scores (e.g., below -0.5): Indicate points\n",
    "    that are strongly considered as anomalies.\n",
    "    Scores approaching 1: Indicate points that are considered\n",
    "    normal.\n",
    "    '''\n",
    "    def predict_anomalies(self, chunk, vectorized_data):\n",
    "        \"\"\"\n",
    "        Detects anomalies in the given data chunk using the trained Isolation Forest model.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "            vectorized_data (sparse matrix): The vectorized data corresponding to the chunk.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with anomaly predictions and scores.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Predict anomalies using the Isolation Forest model\n",
    "            predictions = self.isolation_forest.predict(vectorized_data)\n",
    "            chunk['anomaly'] = predictions\n",
    "\n",
    "            # Calculate anomaly scores (derived from the number of splits required to isolate a data point)\n",
    "            anomaly_scores = self.isolation_forest.decision_function(vectorized_data)\n",
    "            chunk['anomaly_score'] = anomaly_scores\n",
    "\n",
    "            logger.info(\"Anomalies detected and scores assigned\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly detection: {e}\")\n",
    "            return None\n",
    "\n",
    "            \n",
    "    def postprocess_anomalies(self, chunk):\n",
    "        \"\"\"\n",
    "        Post-processes the anomaly predictions by assigning human-readable labels.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with anomaly labels.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Assign human-readable labels based on anomaly predictions\n",
    "            chunk['anomaly_label'] = chunk['anomaly'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')\n",
    "            logger.info(\"Anomaly data post-processed\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly post-processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    '''\n",
    "    Remove rows that are obviously not threats.\n",
    "\n",
    "    In practice, the threshold (x) is usually set to 0. So:\n",
    "    Anomaly Score < 0: Likely an anomaly.\n",
    "    Anomaly Score ≥ 0: Likely not an anomaly (normal).\n",
    "\n",
    "    Using a more stringent threshold like 0.5 instead of 0.0 can help in\n",
    "           identifying anomalies with greater caution.\n",
    "    Anomaly Score < 0.5: Likely an anomaly.\n",
    "    Anomaly Score ≥ 0.5: Likely not an anomaly (normal).\n",
    "    '''\n",
    "    def filter_anomalies(self, chunk):\n",
    "        \"\"\"\n",
    "        Filters the data chunk to include only likely anomalies based on the anomaly score.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The filtered data chunk containing only likely anomalies.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns the original chunk if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filter the chunk to include rows with anomaly scores less than 0.5\n",
    "            chunk = chunk[chunk['anomaly_score'] < 0.5]\n",
    "            logger.info(\"Filtered to likely anomalies only\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly filtering: {e}\")\n",
    "            return None\n",
    "\n",
    "    def categorize_features(self, chunk):\n",
    "        \"\"\"\n",
    "        Encodes categorical features in the data chunk using pre-trained label encoders.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with encoded categorical features.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select categorical columns for encoding\n",
    "            categorical_cols = chunk.select_dtypes(include=['object']).columns\n",
    "\n",
    "            # Apply label encoders to transform categorical features\n",
    "            for col in categorical_cols:\n",
    "                chunk[col] = self.label_encoders[col].transform(chunk[col])\n",
    "            logger.info(\"Features categorized\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during categorization: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def normalize_features(self, chunk):\n",
    "        \"\"\"\n",
    "        Normalizes numerical features in the data chunk using the pre-trained scaler.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with normalized numerical features.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select numerical columns for normalization\n",
    "            numerical_cols = chunk.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "            # Apply the scaler to normalize numerical features\n",
    "            chunk[numerical_cols] = self.scaler.transform(chunk[numerical_cols])\n",
    "            logger.info(\"Features normalized\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during normalization: {e}\")\n",
    "            return None\n",
    "    \n",
    "    '''\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english': \n",
    "    Specifies the pre-trained model to use. In this case, it's a DistilBERT model fine-tuned \n",
    "    on the SST-2 dataset, which is commonly used for sentiment analysis \n",
    "    (e.g., classifying text as positive or negative).\n",
    "    '''\n",
    "    def classify_logs_with_llm(self, chunk):\n",
    "        \"\"\"\n",
    "        Classifies log entries in the given data chunk using a pre-trained language model (LLM).\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with classified log entries.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the text classification pipeline with a pre-trained model\n",
    "            classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "            # Apply the classifier to each log entry and store the classification result\n",
    "            # 'POSITIVE': Indicates that the text has a positive sentiment.\n",
    "            # 'NEGATIVE': Indicates that the text has a negative sentiment.\n",
    "            chunk['classification'] = chunk['text'].apply(lambda x: classifier(x)[0]['label'])\n",
    "\n",
    "            logger.info(\"Logs classified with LLM\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during classification: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    '''\n",
    "    Example of the dictionary:\n",
    "    {\n",
    "        'user_1': {\n",
    "            'count': 7,\n",
    "            'login_attempts': 3.6666666666666665, \n",
    "            'failed_login_attempts': 1.0,\n",
    "            'session_duration': 50.0,\n",
    "            'data_transferred': 110.0,\n",
    "            'access_sensitive_files': 0.3333333333333333, 'count': 3 \n",
    "        },\n",
    "        'user_2': {\n",
    "        ...\n",
    "        },\n",
    "      ...\n",
    "    }\n",
    "    \n",
    "    columns of chunk after aggregate_features(chunk):\n",
    "    user_id  login_attempts  failed_login_attempts  session_duration  data_transferred  access_sensitive_files  count  login_attempts_agg  failed_login_attempts_agg  session_duration_agg  data_transferred_agg  access_sensitive_files_agg\n",
    "    '''\n",
    "    def aggregate_features(self, chunk):\n",
    "        \"\"\" \n",
    "        Aggregates features from the given data chunk and updates the aggregated features dictionary. \n",
    "        Parameters: \n",
    "            chunk (DataFrame): The data chunk to process. \n",
    "        Returns: \n",
    "            DataFrame: The data chunk merged with the aggregated features. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Iterate through each row in the chunk\n",
    "            for index, row in chunk.iterrows():\n",
    "                user_id = row['user_id']\n",
    "                if user_id not in self.aggregated_features:\n",
    "                    # Initialize the aggregated features for a new user_id\n",
    "                    self.aggregated_features[user_id] = {\n",
    "                        'count': 1,\n",
    "                        'login_attempts': row['login_attempts'],\n",
    "                        'failed_login_attempts': row['failed_login_attempts'],\n",
    "                        'session_duration': row['session_duration'],\n",
    "                        'data_transferred': row['data_transferred'],\n",
    "                        'access_sensitive_files': row['access_sensitive_files']\n",
    "                    }\n",
    "                else:\n",
    "                    # Update the aggregated features for an existing user_id\n",
    "                    self.aggregated_features[user_id]['count'] += 1\n",
    "                    for feature in ['login_attempts', 'failed_login_attempts', 'session_duration', 'data_transferred', 'access_sensitive_files']:\n",
    "                        self.aggregated_features[user_id][feature] += (1.0 / self.aggregated_features[user_id]['count']) * (row[feature] - self.aggregated_features[user_id][feature])\n",
    "           \n",
    "            # Convert dictionary to DataFrame\n",
    "            aggregated_df = pd.DataFrame.from_dict(self.aggregated_features, orient='index').reset_index().rename(columns={'index': 'user_id'})\n",
    "           \n",
    "            # Merge aggregated data back with the original chunk\n",
    "            # Perform a left join to ensure the row count of `chunk` remains the same\n",
    "            chunk = pd.merge(chunk, aggregated_df, on='user_id', how='left', suffixes=('', '_agg'))\n",
    "           \n",
    "            logger.info(\"Aggregated features merged with chunk\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during aggregation: {e}\")\n",
    "            return None\n",
    "    \n",
    "       \n",
    "    def augment_data(self, chunk):\n",
    "        \"\"\" \n",
    "        Augments the data in the given chunk by paraphrasing text and modifying numerical values. \n",
    "        Parameters: \n",
    "            chunk (DataFrame): The data chunk to process. \n",
    "        Returns: \n",
    "            DataFrame: The augmented data chunk combined with the original chunk. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            def paraphrase_text(row, num_return_sequences=1):\n",
    "                \"\"\" \n",
    "                Paraphrases the text in the log entry using a text generation model. \n",
    "                Parameters: \n",
    "                    row (Series): A row of data containing the log text. \n",
    "                    num_return_sequences (int): The number of paraphrased sequences to generate. \n",
    "                Returns: \n",
    "                    list: A list of paraphrased text sequences. \n",
    "                \"\"\"\n",
    "                try:\n",
    "                    paraphrases = self.text_generator(row['log_text'], max_length=50, num_return_sequences=num_return_sequences)\n",
    "                    return [p['generated_text'] for p in paraphrases]\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during paraphrasing: {e}\")\n",
    "                    return [row['log_text']]  # Return original text if paraphrasing fails\n",
    "    \n",
    "            def augment_numerical(value):\n",
    "                \"\"\" \n",
    "                Augments a numerical value by multiplying it with a random factor. \n",
    "                Parameters: \n",
    "                    value (float or int): The numerical value to augment. \n",
    "                Returns: \n",
    "                    float or int: The augmented numerical value. \n",
    "                \"\"\"\n",
    "                try:\n",
    "                    augmented_value = value * np.random.uniform(0.9, 1.1)\n",
    "                    if isinstance(value, np.int64):\n",
    "                        return int(round(augmented_value))\n",
    "                    else:\n",
    "                        return augmented_value\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during numerical augmentation: {e}\")\n",
    "                    return value  # Return original value if augmentation fails\n",
    "    \n",
    "            augmented_rows = []\n",
    "            for index, row in chunk.iterrows():\n",
    "                try:\n",
    "                    # Paraphrase the log text\n",
    "                    paraphrased_logs = paraphrase_text(row, num_return_sequences=2)\n",
    "    \n",
    "                    for paraphrased_log in paraphrased_logs:\n",
    "                        new_row = row.copy()\n",
    "    \n",
    "                        # Set new paraphrased log text\n",
    "                        new_row['log_text'] = paraphrased_log\n",
    "    \n",
    "                        # Augment numerical columns\n",
    "                        for col in chunk.select_dtypes(include=['float64', 'int64']).columns:\n",
    "                            new_row[col] = augment_numerical(new_row[col])\n",
    "    \n",
    "                        augmented_rows.append(new_row)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {index}: {e}\")\n",
    "                    continue\n",
    "            augmented_chunk = pd.DataFrame(augmented_rows)\n",
    "            return pd.concat([chunk, augmented_chunk], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error augmenting data: {e}\")\n",
    "            return chunk  # Return original chunk if augmentation fails\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Using row['text'] as Query:\n",
    "    \n",
    "    The detect_threat function uses row['text'] as the query to retrieve relevant documents.\n",
    "    \n",
    "    Document Retrieval:\n",
    "    \n",
    "    The retrieve_documents method uses the query to search for related documents via the Bing Search API.\n",
    "    \n",
    "    Augmented Prompt:\n",
    "    \n",
    "    The retrieved documents are included as additional context in the prompt.\n",
    "    '''\n",
    "    def retrieve_documents(self, query):\n",
    "        \"\"\" \n",
    "        Retrieves relevant documents for the given query using the Bing Search API. \n",
    "        Parameters: \n",
    "            query (str): The search query text. \n",
    "        Returns: \n",
    "            list: A list of retrieved document snippets relevant to the query. \n",
    "        Raises: \n",
    "            None: Logs any exceptions encountered and returns None. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Formulate the search query dynamically based on the log entry text\n",
    "            search_query = f\"System Two Security: {query}\"\n",
    "           \n",
    "            # Make a request to the Bing Search API\n",
    "            response = requests.get(\n",
    "                'https://api.bing.microsoft.com/v7.0/search',\n",
    "                params={'q': search_query, 'count': 5},\n",
    "                headers={'Ocp-Apim-Subscription-Key': 'YOUR_BING_SEARCH_API_KEY'}\n",
    "            )\n",
    "           \n",
    "            # Parse the JSON response\n",
    "            search_results = response.json()\n",
    "           \n",
    "            # Extract relevant documents (simplified for illustration)\n",
    "            retrieved_docs = [result['snippet'] for result in search_results.get('webPages', {}).get('value', [])]\n",
    "           \n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Primary Focus on Log Entry: The prompt focuses on the original log entry and its attributes.\n",
    "    \n",
    "    Additional Context: The retrieved documents are added as supplementary context, not the main focus.\n",
    "    \n",
    "    Balanced Prompt: This ensures that the decision-making process revolves around the log entry while benefiting from additional context.\n",
    "    '''\n",
    "    def detect_threats(self, chunk):\n",
    "        \"\"\"\n",
    "        Detects threats in the given data chunk using a generative AI model.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The processed data chunk with threat detection results.\n",
    "        \"\"\"\n",
    "        def detect_threat(row):\n",
    "            \"\"\"\n",
    "            Detects threats for a single row of data.\n",
    "            Parameters:\n",
    "                row (Series): A row of data from the chunk.\n",
    "            Returns:\n",
    "                str: 'Threat' if the generated text indicates a threat, 'Normal' otherwise.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Use the log entry text as the query\n",
    "                query = row['log_text']\n",
    "                \n",
    "                # Retrieve relevant documents for the log entry\n",
    "                retrieved_docs = self.retrieve_documents(query)\n",
    "                docs_text = \"\\n\".join(retrieved_docs)\n",
    "                \n",
    "                # Create the augmented prompt\n",
    "                prompt = (\n",
    "                    f\"Log entry: {row['text']}\\n\"\n",
    "                    f\"Classification: {row['classification']}\\n\"\n",
    "                    f\"Predictive Anomaly Label: {row['anomaly_label']}\\n\"\n",
    "                    \"Is this a threat?\\n\"\n",
    "                    f\"Additional context:\\n{docs_text}\"\n",
    "                )\n",
    "                \n",
    "                # Generate text using text_generator as the threat detection model\n",
    "                generated_text = self.text_generator(prompt, \n",
    "                                                             max_length=50, \n",
    "                                                             num_return_sequences=1)[0]['generated_text']\n",
    "                return 'Threat' if 'threat' in generated_text.lower() else 'Normal'\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during threat detection for row: {row.name} - {e}\")\n",
    "                return 'Error'\n",
    "    \n",
    "        try:\n",
    "            # Apply the detect_threat function to each row in the chunk\n",
    "            chunk['threat_detection'] = chunk.apply(detect_threat, axis=1)\n",
    "            logger.info(\"Threats detected using generative AI\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error applying threat detection: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_chunk(self, chunk):\n",
    "        \"\"\"\n",
    "        Saves the processed data chunk to the output file.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to save.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunk.to_csv(self.output_file, \n",
    "                         mode='a', \n",
    "                         index=False, \n",
    "                         header=not pd.io.common.file_exists(self.output_file))\n",
    "            logger.info(f\"Chunk saved to {self.output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving chunk: {e}\")\n",
    "    \n",
    "    def save_aggregated_features(self):\n",
    "        \"\"\"\n",
    "        Saves the aggregated features to a file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.aggregated_features, self.aggregated_features_file)\n",
    "            logger.info(\"Aggregated features saved.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving aggregated features: {e}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" \n",
    "        Runs the data processing pipeline in chunks. This method loads the preprocessors, aggregated features, etc. It then processes the input data in chunks, applying various \n",
    "        data transformations and saving results. \n",
    "        \"\"\"\n",
    "        self.load_missing_data_replacement()\n",
    "        self.load_isolation_forest()\n",
    "        self.load_preprocessors()\n",
    "        self.load_aggregated_features()\n",
    "        self.load_fine_tuned_model()\n",
    "        # If we need to fine-tune GPT-3.5-Turbo on the specific dataset using OpenAI’s fine-tuning API. \n",
    "        # Once fine-tuned, save fine_tune_id locally and upload it to get fine_tuned_model.\n",
    "        # Additionally, we may implement rag_text_generation(query) to use RAG for better predictive performance.\n",
    "        fine_tuned_model = self.load_fine_tuned_model()  \n",
    "        self.text_generator = pipeline('text-generation', model=fine_tuned_model)\n",
    "       \n",
    "        try:\n",
    "            chunks = pd.read_csv(self.input_file, chunksize=self.chunk_size)\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    if (chunk := self.simple_filter(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.preprocess_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (vectorized_data := self.vectorize_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.predict_anomalies(chunk, vectorized_data)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.postprocess_anomalies(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.filter_anomalies(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.categorize_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.normalize_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.classify_logs_with_llm(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.aggregate_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.augment_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.detect_threats(chunk)) is None:\n",
    "                        continue\n",
    "                    self.save_chunk(chunk)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing chunk: {e}\")\n",
    "            self.save_aggregated_features()  # Ensure to call this method outside the inner try-except block\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Error: File not found.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.error(\"Error: No data in the file.\")\n",
    "        except pd.errors.ParserError:\n",
    "            logger.error(\"Error: Parsing error.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data in chunks: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = current_directory + '/input/'\n",
    "    output_dir = current_directory + '/output/'\n",
    "    \n",
    "    detector = CybersecurityDataTransformation(input_dir + '/input_data.csv', \n",
    "                                               output_dir + '/detection_results.csv', \n",
    "                                               output_dir + '/missing_dict.pkl', \n",
    "                                               output_dir + '/vectorizer.pkl', \n",
    "                                               output_dir + '/isolation_forest_model.pkl', \n",
    "                                               output_dir + '/scaler.pkl', \n",
    "                                               output_dir + '/label_encoders.pkl', \n",
    "                                               output_dir + '/aggregated_features.pkl', \n",
    "                                               output_dir + 'fine_tune_id.txt',\n",
    "                                               chunk_size=1000)\n",
    "    try:\n",
    "        detector.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing run method: {e}\")\n",
    "        sys.exit(1) # Exit the program with a status code 1 indicating an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6d075-2eff-48e9-bf58-cbcb57f74247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
